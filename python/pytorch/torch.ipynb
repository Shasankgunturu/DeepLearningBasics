{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Basics\n",
    "x = torch.empty(3) # vector of 3\n",
    "y = torch.empty(2,2) # 2d vector\n",
    "# print (x, y)\n",
    "# print (x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,2)\n",
    "# torch.zeros(2)\n",
    "# torch.ones(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3658)\n",
      "0.3658078908920288\n"
     ]
    }
   ],
   "source": [
    "print(x[1,1])\n",
    "print(x[1,1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# pytorch to numpy\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) #if a changes, b also changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "a = a+1\n",
    "print(a)\n",
    "print(b) #here b doesnt change as no gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)  #by default is set to false, this means that we will have to optimize this variable and\n",
    "                                       #gradient needs to be calculated later\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3., 3.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18., 18., 18., 18., 18.], grad_fn=<MulBackward0>)\n",
      "tensor(18., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*2\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4000, 2.4000, 2.4000, 2.4000, 2.4000])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(2):\n",
    "    model_out = (weights*3).sum()\n",
    "    model_out.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back propogation\n",
    "\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(5.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-12.)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 2.800, loss = 70.000\n",
      "epoch 2: w = 1.680, loss = 11.200\n",
      "epoch 3: w = 2.128, loss = 1.792\n",
      "epoch 4: w = 1.949, loss = 0.287\n",
      "epoch 5: w = 2.020, loss = 0.046\n",
      "epoch 6: w = 1.992, loss = 0.007\n",
      "epoch 7: w = 2.003, loss = 0.001\n",
      "epoch 8: w = 1.999, loss = 0.000\n",
      "epoch 9: w = 2.001, loss = 0.000\n",
      "epoch 10: w = 2.000, loss = 0.000\n",
      "epoch 11: w = 2.000, loss = 0.000\n",
      "epoch 12: w = 2.000, loss = 0.000\n",
      "epoch 13: w = 2.000, loss = 0.000\n",
      "epoch 14: w = 2.000, loss = 0.000\n",
      "epoch 15: w = 2.000, loss = 0.000\n",
      "epoch 16: w = 2.000, loss = 0.000\n",
      "epoch 17: w = 2.000, loss = 0.000\n",
      "epoch 18: w = 2.000, loss = 0.000\n",
      "epoch 19: w = 2.000, loss = 0.000\n",
      "epoch 20: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent from scratch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# y = 2*x\n",
    "X = np.array([1,2,4,7], dtype=np.float32)\n",
    "Y = np.array([2,4,8,14], dtype=np.float32)\n",
    "\n",
    "w = 0\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y, y_predicted):\n",
    "    return np.mean((y_predicted - y)**2)\n",
    "\n",
    "#gradient\n",
    "# L = 1/N * ((w*x - y)**2)\n",
    "# dL/dw = 1/N * (2*(w*x - y) * x)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return -np.dot(2*x, y-y_predicted).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    #gradient\n",
    "    dw = gradient(X, Y, y_predicted)\n",
    "\n",
    "    #learning rate\n",
    "    w -= learning_rate*dw\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.3f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.700, loss = 70.000\n",
      "epoch 4: w = 1.643, loss = 5.279\n",
      "epoch 7: w = 1.902, loss = 0.398\n",
      "epoch 10: w = 1.973, loss = 0.030\n",
      "epoch 13: w = 1.993, loss = 0.002\n",
      "epoch 16: w = 1.998, loss = 0.000\n",
      "epoch 19: w = 1.999, loss = 0.000\n",
      "epoch 22: w = 2.000, loss = 0.000\n",
      "epoch 25: w = 2.000, loss = 0.000\n",
      "epoch 28: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent using torch\n",
    "\n",
    "# y = 2*x\n",
    "X = torch.tensor([1,2,4,7], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,8,14], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "#gradient\n",
    "# L = 1/N * ((w*x - y)**2)\n",
    "# dL/dw = 1/N * (2*(w*x - y) * x)\n",
    "def gradient(l, weights):\n",
    "    l.backward()\n",
    "    return weights.grad\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 30\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    #gradient\n",
    "    dw = gradient(l, w)\n",
    "\n",
    "    #learning rate\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*dw\n",
    " \n",
    "    # zero the gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch%3 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.3f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design model - input, output size, forward pass\n",
    "2. Construct loss and the optimizer\n",
    "3. Training loop \n",
    "    - forward pass: Compute prediction\n",
    "    - backward pass and gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.700, loss = 70.000\n",
      "epoch 4: w = 1.643, loss = 5.279\n",
      "epoch 7: w = 1.902, loss = 0.398\n",
      "epoch 10: w = 1.973, loss = 0.030\n",
      "epoch 13: w = 1.993, loss = 0.002\n",
      "epoch 16: w = 1.998, loss = 0.000\n",
      "epoch 19: w = 1.999, loss = 0.000\n",
      "epoch 22: w = 2.000, loss = 0.000\n",
      "epoch 25: w = 2.000, loss = 0.000\n",
      "epoch 28: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# y = 2*x\n",
    "X = torch.tensor([1,2,4,7], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,8,14], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.01\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([w], learning_rate)\n",
    "#gradient\n",
    "# L = 1/N * ((w*x - y)**2)\n",
    "# dL/dw = 1/N * (2*(w*x - y) * x)\n",
    "def gradient(l, weights):\n",
    "    l.backward()\n",
    "    return weights.grad\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "#Training\n",
    "n_iters = 30\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    #gradient\n",
    "    dw = gradient(l, w)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch%3 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.3f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -5.026\n",
      "epoch 1: w = 0.122, loss = 158.079\n",
      "epoch 61: w = 1.970, loss = 0.007\n",
      "epoch 121: w = 1.979, loss = 0.003\n",
      "epoch 181: w = 1.985, loss = 0.002\n",
      "epoch 241: w = 1.989, loss = 0.001\n",
      "epoch 301: w = 1.992, loss = 0.000\n",
      "epoch 361: w = 1.995, loss = 0.000\n",
      "epoch 421: w = 1.996, loss = 0.000\n",
      "epoch 481: w = 1.997, loss = 0.000\n",
      "epoch 541: w = 1.998, loss = 0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# only one layer in the linear regression model\n",
    "# which is also defined by pytorch - model = nn.Linear()\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# y = 2*x\n",
    "# pytorch model forward needs 2d - each row is a sample and each column is a feature\n",
    "X = torch.tensor([[1],[2],[4],[7]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[8],[14]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "#model prediction\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "#loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "#gradient\n",
    "# L = 1/N * ((w*x - y)**2)\n",
    "# dL/dw = 1/N * (2*(w*x - y) * x)\n",
    "def gradient(l, weights):\n",
    "    l.backward()\n",
    "    return weights.grad\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "#Training\n",
    "n_iters = 600\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    #gradient\n",
    "    dw = gradient(l, w)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch%60 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.3f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -4.476\n",
      "epoch 1: w = 0.038, loss = 146.728\n",
      "epoch 61: w = 1.862, loss = 0.139\n",
      "epoch 121: w = 1.903, loss = 0.070\n",
      "epoch 181: w = 1.931, loss = 0.035\n",
      "epoch 241: w = 1.951, loss = 0.017\n",
      "epoch 301: w = 1.966, loss = 0.009\n",
      "epoch 361: w = 1.976, loss = 0.004\n",
      "epoch 421: w = 1.983, loss = 0.002\n",
      "epoch 481: w = 1.988, loss = 0.001\n",
      "epoch 541: w = 1.991, loss = 0.001\n",
      "Prediction after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# custom linear regression\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dimensions, output_dimensions):\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dimensions, output_dimensions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "# y = 2*x\n",
    "# pytorch model forward needs 2d - each row is a sample and each column is a feature\n",
    "X = torch.tensor([[1],[2],[4],[7]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[8],[14]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "#model prediction\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "#loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "#gradient\n",
    "# L = 1/N * ((w*x - y)**2)\n",
    "# dL/dw = 1/N * (2*(w*x - y) * x)\n",
    "def gradient(l, weights):\n",
    "    l.backward()\n",
    "    return weights.grad\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "#Training\n",
    "n_iters = 600\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    #gradient\n",
    "    dw = gradient(l, w)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch%60 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.3f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
